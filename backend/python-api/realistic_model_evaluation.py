import os
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Deep Learning Libraries
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier

# Machine Learning Libraries
from sklearn.model_selection import (train_test_split, cross_val_score, 
                                   StratifiedKFold, GridSearchCV, RandomizedSearchCV)
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import (confusion_matrix, classification_report,
                           precision_score, recall_score, f1_score, accuracy_score,
                           roc_auc_score, roc_curve)
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

# Hyperparameter tuning
import optuna

class RealisticHepatitisEvaluation:
    """
    Realistic Hepatitis Model Evaluation with Cross-Validation and Real Data Testing
    Addresses overfitting concerns from synthetic data
    """
    
    def __init__(self, output_dir='realistic_evaluation_outputs'):
        self.output_dir = output_dir
        self.create_directory_structure()
        
        # Initialize containers
        self.synthetic_data = None
        self.real_data = None
        self.model = None
        self.scaler = None
        self.label_encoder = None
        self.feature_cols = []
        self.results = {}
        
        # Set style for plots
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        print("üîç REALISTIC HEPATITIS MODEL EVALUATION")
        print("=" * 60)
        print("Addressing Overfitting with Cross-Validation & Real Data Testing")
        print("=" * 60)

    def create_directory_structure(self):
        """Create comprehensive directory structure"""
        directories = [
            self.output_dir,
            f'{self.output_dir}/figures',
            f'{self.output_dir}/models',
            f'{self.output_dir}/results',
            f'{self.output_dir}/cross_validation',
            f'{self.output_dir}/real_data_testing'
        ]
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
        print(f"üìÅ Created directory structure in: {self.output_dir}")

    def load_datasets(self):
        """Load both synthetic and real datasets"""
        print("\n" + "="*50)
        print("LOADING DATASETS")
        print("="*50)
        
        # Load synthetic dataset
        synthetic_path = '../data/hepatitis_dataset_5k.csv'
        if os.path.exists(synthetic_path):
            self.synthetic_data = pd.read_csv(synthetic_path)
            print(f"‚úÖ Synthetic dataset loaded: {self.synthetic_data.shape}")
        else:
            print(f"‚ùå Synthetic dataset not found at: {synthetic_path}")
            return False
        
        # Load real dataset (if available)
        real_data_paths = [
            '../data/hepatitis_dataset_R.csv',
            '../data/hepatitis_dataset_5k.csv',  # Use this as "real" for comparison
            '../hepatitis_thesis_outputs/data/dataset_sample.csv'
        ]
        
        for path in real_data_paths:
            if os.path.exists(path):
                self.real_data = pd.read_csv(path)
                print(f"‚úÖ Real dataset loaded: {self.real_data.shape}")
                break
        else:
            print("‚ö†Ô∏è No real dataset found, will use synthetic data with cross-validation")
            self.real_data = self.synthetic_data.copy()
        
        return True

    def prepare_data_for_cross_validation(self, data):
        """Prepare data for cross-validation"""
        print("\nüîß Preparing data for cross-validation...")
        
        # Create a copy for processing
        df = data.copy()
        
        # Handle missing values
        for col in df.columns:
            if df[col].dtype == 'object':
                df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown')
            else:
                df[col] = df[col].fillna(df[col].median())
        
        # Create target variable
        if 'HepatitisType' in df.columns:
            target_col = 'HepatitisType'
            label_encoder = LabelEncoder()
            y = label_encoder.fit_transform(df[target_col].astype(str))
        else:
            print("‚ùå No HepatitisType column found")
            return None, None, None
        
        # Feature engineering (same as training)
        symptoms_list = [
            'ascites', 'swelling', 'spider angiomas', 'itchy skin', 'jaundice',
            'dark-colored urine', 'bleeding easily', 'weight loss', 'fatigue',
            'confusion', 'not wanting to eat', 'bruising easily', 'nausea',
            'vomiting', 'fever', 'abdominal pain', 'joint pain', 'loss of appetite'
        ]
        
        for symptom in symptoms_list:
            df[f'has_{symptom.replace(" ", "_").replace("-", "_")}'] = df['Symptoms'].str.contains(
                symptom, case=False, na=False).astype(int)
        
        # Hepatitis-specific signatures
        df['hepatitis_a_signature'] = (
            (df['has_abdominal_pain'] | df['has_nausea'] | df['has_vomiting']) &
            ~df['has_weight_loss'] & ~df['has_ascites']
        ).astype(int)
        
        df['hepatitis_c_signature'] = (
            (df['has_weight_loss'] | df['has_ascites'] | df['has_spider_angiomas']) &
            (df['has_dark_colored_urine'] | df['has_bleeding_easily'])
        ).astype(int)
        
        # Date features
        if 'DiagnosisDate' in df.columns:
            df['DiagnosisDate'] = pd.to_datetime(df['DiagnosisDate'])
            df['DiagnosisYear'] = df['DiagnosisDate'].dt.year
            df['DiagnosisMonth'] = df['DiagnosisDate'].dt.month
            df['DiagnosisDay'] = df['DiagnosisDate'].dt.day
            df['DiagnosisDayOfWeek'] = df['DiagnosisDate'].dt.dayofweek
        
        # Severity encoding
        if 'Severity' in df.columns:
            severity_mapping = {'Mild': 1, 'Moderate': 2, 'Severe': 3}
            df['Severity_encoded'] = df['Severity'].map(severity_mapping).fillna(2)
        
        # Categorical encoding
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            if col not in [target_col, 'Symptoms', 'DiagnosisDate']:
                le = LabelEncoder()
                unique_values = list(df[col].unique()) + ['UNK']
                le.fit(unique_values)
                df[col + '_encoded'] = le.transform(df[col].astype(str))
        
        # Select features
        feature_cols = []
        for col in df.columns:
            if (col.endswith('_encoded') or
                col.startswith('has_') or
                col.endswith('_signature') or
                (df[col].dtype in ['int64', 'float64'] and
                 col not in [target_col, 'PatientID'] and
                 not col.startswith('Diagnosis') and
                 col != 'Symptoms')):
                feature_cols.append(col)
        
        X = df[feature_cols]
        
        print(f"üìä Features: {X.shape[1]}, Samples: {X.shape[0]}")
        print(f"üéØ Target classes: {label_encoder.classes_}")
        
        return X, y, feature_cols

    def create_keras_model(self, input_dim, dropout_rate=0.3):
        """Create a Keras model for cross-validation"""
        def create_model():
            model = tf.keras.Sequential([
                Dense(64, activation='relu', input_shape=(input_dim,)),
                BatchNormalization(),
                Dropout(dropout_rate),
                Dense(32, activation='relu'),
                BatchNormalization(),
                Dropout(dropout_rate),
                Dense(16, activation='relu'),
                Dropout(dropout_rate),
                Dense(1, activation='sigmoid')
            ])
            
            model.compile(
                optimizer=Adam(learning_rate=0.001),
                loss='binary_crossentropy',
                metrics=['accuracy']
            )
            return model
        
        return create_model

    def perform_cross_validation(self, X, y, feature_cols):
        """Perform comprehensive cross-validation"""
        print("\n" + "="*50)
        print("CROSS-VALIDATION EVALUATION")
        print("="*50)
        
        # Initialize scaler
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Store results
        cv_results = {}
        
        # 1. Traditional ML Models with Cross-Validation
        print("ü§ñ Traditional ML Models Cross-Validation...")
        
        models = {
            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
            'SVM': SVC(probability=True, random_state=42),
            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)
        }
        
        # Perform 5-fold cross-validation
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        for name, model in models.items():
            print(f"   Testing {name}...")
            
            # Cross-validation scores
            cv_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='accuracy')
            
            # Train on full data for detailed metrics
            model.fit(X_scaled, y)
            y_pred = model.predict(X_scaled)
            
            cv_results[name] = {
                'cv_scores': cv_scores,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'accuracy': accuracy_score(y, y_pred),
                'precision': precision_score(y, y_pred, average='weighted'),
                'recall': recall_score(y, y_pred, average='weighted'),
                'f1': f1_score(y, y_pred, average='weighted')
            }
            
            print(f"     CV Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
        
        # 2. Neural Network Cross-Validation
        print("\nüß† Neural Network Cross-Validation...")
        
        # Create Keras wrapper for scikit-learn compatibility
        input_dim = X_scaled.shape[1]
        keras_model = KerasClassifier(
            build_fn=self.create_keras_model(input_dim),
            epochs=50,
            batch_size=32,
            verbose=0
        )
        
        # Cross-validation for neural network
        nn_cv_scores = cross_val_score(keras_model, X_scaled, y, cv=cv, scoring='accuracy')
        
        # Train on full data for detailed metrics
        keras_model.fit(X_scaled, y)
        y_pred_nn = keras_model.predict(X_scaled)
        
        cv_results['Neural Network'] = {
            'cv_scores': nn_cv_scores,
            'cv_mean': nn_cv_scores.mean(),
            'cv_std': nn_cv_scores.std(),
            'accuracy': accuracy_score(y, y_pred_nn),
            'precision': precision_score(y, y_pred_nn, average='weighted'),
            'recall': recall_score(y, y_pred_nn, average='weighted'),
            'f1': f1_score(y, y_pred_nn, average='weighted')
        }
        
        print(f"   CV Accuracy: {nn_cv_scores.mean():.3f} (+/- {nn_cv_scores.std() * 2:.3f})")
        
        # 3. Save cross-validation results
        with open(f'{self.output_dir}/cross_validation/cv_results.pkl', 'wb') as f:
            pickle.dump(cv_results, f)
        
        # 4. Plot cross-validation results
        self.plot_cross_validation_results(cv_results)
        
        return cv_results, scaler, feature_cols

    def plot_cross_validation_results(self, cv_results):
        """Plot cross-validation results"""
        print("\nüìä Plotting cross-validation results...")
        
        # Create figure with subplots
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Cross-validation accuracy comparison
        model_names = list(cv_results.keys())
        cv_means = [cv_results[name]['cv_mean'] for name in model_names]
        cv_stds = [cv_results[name]['cv_std'] for name in model_names]
        
        bars = axes[0, 0].bar(model_names, cv_means, yerr=cv_stds, capsize=5, 
                              color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'], alpha=0.8)
        
        # Add value labels
        for bar, mean, std in zip(bars, cv_means, cv_stds):
            axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{mean:.3f}¬±{std:.3f}', ha='center', va='bottom', fontweight='bold')
        
        axes[0, 0].set_title('Cross-Validation Accuracy Comparison', fontsize=14, fontweight='bold')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].set_ylim(0, 1)
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # 2. Detailed metrics comparison
        metrics = ['accuracy', 'precision', 'recall', 'f1']
        metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        
        x = np.arange(len(model_names))
        width = 0.2
        
        for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
            values = [cv_results[name][metric] for name in model_names]
            axes[0, 1].bar(x + i*width, values, width, label=metric_name, alpha=0.8)
        
        axes[0, 1].set_title('Detailed Metrics Comparison', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('Models')
        axes[0, 1].set_ylabel('Score')
        axes[0, 1].set_xticks(x + width * 1.5)
        axes[0, 1].set_xticklabels(model_names, rotation=45)
        axes[0, 1].legend()
        axes[0, 1].set_ylim(0, 1)
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. Cross-validation score distributions
        for i, name in enumerate(model_names):
            scores = cv_results[name]['cv_scores']
            axes[1, 0].hist(scores, alpha=0.7, label=name, bins=10)
        
        axes[1, 0].set_title('Cross-Validation Score Distributions', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Accuracy Score')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. Model performance summary
        summary_data = []
        for name in model_names:
            summary_data.append([
                name,
                f"{cv_results[name]['cv_mean']:.3f}",
                f"{cv_results[name]['cv_std']:.3f}",
                f"{cv_results[name]['precision']:.3f}",
                f"{cv_results[name]['recall']:.3f}",
                f"{cv_results[name]['f1']:.3f}"
            ])
        
        # Create table
        axes[1, 1].axis('tight')
        axes[1, 1].axis('off')
        table = axes[1, 1].table(cellText=summary_data,
                                colLabels=['Model', 'CV Mean', 'CV Std', 'Precision', 'Recall', 'F1'],
                                cellLoc='center',
                                loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1.2, 1.5)
        axes[1, 1].set_title('Model Performance Summary', fontsize=14, fontweight='bold', pad=20)
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/figures/cross_validation_results.png', dpi=300, bbox_inches='tight')
        plt.show()
        print(f"üìä Cross-validation plots saved to: {self.output_dir}/figures/cross_validation_results.png")

    def test_on_real_data(self, cv_results, scaler, feature_cols):
        """Test models on real data to check generalization"""
        print("\n" + "="*50)
        print("REAL DATA TESTING")
        print("="*50)
        
        if self.real_data is None:
            print("‚ùå No real data available for testing")
            return None
        
        # Prepare real data
        X_real, y_real, _ = self.prepare_data_for_cross_validation(self.real_data)
        if X_real is None:
            return None
        
        # Ensure feature compatibility
        missing_features = set(feature_cols) - set(X_real.columns)
        if missing_features:
            print(f"‚ö†Ô∏è Adding missing features: {missing_features}")
            for feature in missing_features:
                X_real[feature] = 0
        
        # Select and scale features
        X_real = X_real[feature_cols]
        X_real_scaled = scaler.transform(X_real)
        
        # Test each model
        real_data_results = {}
        
        for name, results in cv_results.items():
            print(f"\nüß™ Testing {name} on real data...")
            
            if name == 'Neural Network':
                # For neural network, we need to retrain on the synthetic data first
                print("   Retraining neural network on synthetic data...")
                X_syn, y_syn, _ = self.prepare_data_for_cross_validation(self.synthetic_data)
                X_syn = X_syn[feature_cols]
                X_syn_scaled = scaler.transform(X_syn)
                
                # Create and train neural network
                input_dim = X_syn_scaled.shape[1]
                nn_model = self.create_keras_model(input_dim)()
                nn_model.fit(X_syn_scaled, y_syn, epochs=50, batch_size=32, verbose=0)
                
                # Predict on real data
                y_pred = (nn_model.predict(X_real_scaled, verbose=0) > 0.5).astype(int)
            else:
                # For traditional ML models, retrain on synthetic data
                print("   Retraining on synthetic data...")
                X_syn, y_syn, _ = self.prepare_data_for_cross_validation(self.synthetic_data)
                X_syn = X_syn[feature_cols]
                X_syn_scaled = scaler.transform(X_syn)
                
                # Retrain model
                model = results['model'] if 'model' in results else models[name]
                model.fit(X_syn_scaled, y_syn)
                
                # Predict on real data
                y_pred = model.predict(X_real_scaled)
            
            # Calculate metrics
            accuracy = accuracy_score(y_real, y_pred)
            precision = precision_score(y_real, y_pred, average='weighted')
            recall = recall_score(y_real, y_pred, average='weighted')
            f1 = f1_score(y_real, y_pred, average='weighted')
            
            real_data_results[name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'predictions': y_pred,
                'true_labels': y_real
            }
            
            print(f"     Real Data Accuracy: {accuracy:.3f}")
            print(f"     Real Data Precision: {precision:.3f}")
            print(f"     Real Data Recall: {recall:.3f}")
            print(f"     Real Data F1: {f1:.3f}")
        
        # Save real data results
        with open(f'{self.output_dir}/real_data_testing/real_data_results.pkl', 'wb') as f:
            pickle.dump(real_data_results, f)
        
        # Plot real data performance comparison
        self.plot_real_data_comparison(cv_results, real_data_results)
        
        return real_data_results

    def plot_real_data_comparison(self, cv_results, real_data_results):
        """Plot comparison between synthetic and real data performance"""
        print("\nüìä Plotting real data comparison...")
        
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        # 1. Accuracy comparison: Synthetic vs Real
        model_names = list(cv_results.keys())
        synthetic_acc = [cv_results[name]['cv_mean'] for name in model_names]
        real_acc = [real_data_results[name]['accuracy'] for name in model_names]
        
        x = np.arange(len(model_names))
        width = 0.35
        
        bars1 = axes[0].bar(x - width/2, synthetic_acc, width, label='Synthetic Data (CV)', 
                           color='#4ECDC4', alpha=0.8)
        bars2 = axes[0].bar(x + width/2, real_acc, width, label='Real Data', 
                           color='#FF6B6B', alpha=0.8)
        
        # Add value labels
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                axes[0].text(bar.get_x() + bar.get_width()/2, height + 0.01,
                           f'{height:.3f}', ha='center', va='bottom', fontsize=8)
        
        axes[0].set_title('Synthetic vs Real Data Performance', fontsize=14, fontweight='bold')
        axes[0].set_xlabel('Models')
        axes[0].set_ylabel('Accuracy')
        axes[0].set_xticks(x)
        axes[0].set_xticklabels(model_names, rotation=45)
        axes[0].legend()
        axes[0].set_ylim(0, 1)
        axes[0].grid(True, alpha=0.3)
        
        # 2. Performance degradation analysis
        degradation = [(syn - real) for syn, real in zip(synthetic_acc, real_acc)]
        colors = ['red' if deg > 0.1 else 'orange' if deg > 0.05 else 'green' for deg in degradation]
        
        bars = axes[1].bar(model_names, degradation, color=colors, alpha=0.8)
        
        # Add value labels
        for bar, deg in zip(bars, degradation):
            axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                        f'{deg:.3f}', ha='center', va='bottom', fontweight='bold')
        
        axes[1].set_title('Performance Degradation (Synthetic - Real)', fontsize=14, fontweight='bold')
        axes[1].set_xlabel('Models')
        axes[1].set_ylabel('Accuracy Drop')
        axes[1].set_xticklabels(model_names, rotation=45)
        axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)
        axes[1].grid(True, alpha=0.3)
        
        # Add color legend
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='green', alpha=0.8, label='Good Generalization (<5% drop)'),
            Patch(facecolor='orange', alpha=0.8, label='Moderate Overfitting (5-10% drop)'),
            Patch(facecolor='red', alpha=0.8, label='Severe Overfitting (>10% drop)')
        ]
        axes[1].legend(handles=legend_elements, loc='upper right')
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/figures/real_data_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
        print(f"üìä Real data comparison plots saved to: {self.output_dir}/figures/real_data_comparison.png")

    def generate_realistic_report(self, cv_results, real_data_results):
        """Generate a comprehensive realistic evaluation report"""
        print("\n" + "="*50)
        print("GENERATING REALISTIC EVALUATION REPORT")
        print("="*50)
        
        # Calculate overall statistics
        synthetic_means = [cv_results[name]['cv_mean'] for name in cv_results.keys()]
        real_means = [real_data_results[name]['accuracy'] for name in real_data_results.keys()]
        
        avg_synthetic = np.mean(synthetic_means)
        avg_real = np.mean(real_means)
        avg_degradation = avg_synthetic - avg_real
        
        # Determine overfitting severity
        if avg_degradation < 0.05:
            overfitting_level = "MINIMAL"
            recommendation = "Model generalizes well to real data"
        elif avg_degradation < 0.10:
            overfitting_level = "MODERATE"
            recommendation = "Some overfitting detected, consider regularization"
        else:
            overfitting_level = "SEVERE"
            recommendation = "Significant overfitting - model needs retraining"
        
        # Create comprehensive report
        report = f"""
REALISTIC HEPATITIS MODEL EVALUATION REPORT
============================================
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

OVERALL ASSESSMENT
------------------
Average Synthetic Data Performance: {avg_synthetic:.3f}
Average Real Data Performance: {avg_real:.3f}
Performance Degradation: {avg_degradation:.3f}
Overfitting Level: {overfitting_level}

RECOMMENDATION
--------------
{recommendation}

DETAILED MODEL PERFORMANCE
--------------------------
"""
        
        for name in cv_results.keys():
            syn_acc = cv_results[name]['cv_mean']
            real_acc = real_data_results[name]['accuracy']
            degradation = syn_acc - real_acc
            
            report += f"""
{name}:
  Synthetic Data (CV): {syn_acc:.3f}
  Real Data: {real_acc:.3f}
  Degradation: {degradation:.3f}
  Status: {'‚úÖ Good' if degradation < 0.05 else '‚ö†Ô∏è Moderate' if degradation < 0.10 else '‚ùå Poor'}
"""
        
        report += f"""

CROSS-VALIDATION INSIGHTS
-------------------------
- 5-fold stratified cross-validation performed
- Models tested on both synthetic and real data
- Performance degradation indicates overfitting severity

REALISTIC EXPECTATIONS
----------------------
For medical diagnosis models:
- Excellent: 90-95% accuracy
- Good: 85-90% accuracy  
- Acceptable: 80-85% accuracy
- Below 80%: Needs improvement

CONCLUSION
----------
Your model shows {overfitting_level.lower()} overfitting with a {avg_degradation:.1%} performance drop
from synthetic to real data. {recommendation}

Next Steps:
1. Implement stronger regularization
2. Use more diverse training data
3. Reduce model complexity
4. Consider ensemble methods
"""
        
        # Save report
        with open(f'{self.output_dir}/results/realistic_evaluation_report.txt', 'w') as f:
            f.write(report)
        
        print(report)
        print(f"\nüìÑ Report saved to: {self.output_dir}/results/realistic_evaluation_report.txt")
        
        return report

    def run_complete_evaluation(self):
        """Run the complete realistic evaluation pipeline"""
        print("\nüöÄ STARTING REALISTIC EVALUATION PIPELINE")
        print("=" * 60)
        
        try:
            # Step 1: Load datasets
            if not self.load_datasets():
                print("‚ùå Failed to load datasets")
                return False
            
            # Step 2: Prepare synthetic data for cross-validation
            X_syn, y_syn, feature_cols = self.prepare_data_for_cross_validation(self.synthetic_data)
            if X_syn is None:
                print("‚ùå Failed to prepare synthetic data")
                return False
            
            # Step 3: Perform cross-validation
            cv_results, scaler, feature_cols = self.perform_cross_validation(X_syn, y_syn, feature_cols)
            
            # Step 4: Test on real data
            real_data_results = self.test_on_real_data(cv_results, scaler, feature_cols)
            
            # Step 5: Generate realistic report
            self.generate_realistic_report(cv_results, real_data_results)
            
            print("\n‚úÖ REALISTIC EVALUATION PIPELINE COMPLETED")
            print("=" * 60)
            print(f"üìä All results saved in: {self.output_dir}")
            print(f"üìÑ Report: {self.output_dir}/results/realistic_evaluation_report.txt")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error in evaluation pipeline: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """Main execution function"""
    print("üîç INITIALIZING REALISTIC HEPATITIS MODEL EVALUATION...")
    print("=" * 60)
    
    # Initialize evaluation system
    evaluator = RealisticHepatitisEvaluation()
    
    try:
        # Run complete evaluation
        print("\nüîÑ RUNNING COMPLETE EVALUATION...")
        success = evaluator.run_complete_evaluation()
        
        if success:
            print("\n" + "üéâ" * 20)
            print("REALISTIC EVALUATION COMPLETED!")
            print("üéâ" * 20)
            print(f"\nAll results saved in: {evaluator.output_dir}")
        else:
            print("‚ùå Evaluation pipeline failed")
            
    except Exception as e:
        print(f"‚ùå Error in main execution: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
